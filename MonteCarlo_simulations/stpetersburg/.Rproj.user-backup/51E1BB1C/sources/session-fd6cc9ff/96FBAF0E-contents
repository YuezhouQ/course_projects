---
title: "501Final_Project"
format:
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

# 1. Set-up & Data Loading

```{r}
library(data.table)
library(dplyr)
library(stringr)
library(purrr)
library(nnet) 
```

## 1.1 Load Training Data

```{r}
out_dir <- path.expand("~/Desktop/Projects/ADHD200/ADHD200_data")

if (!dir.exists(out_dir)) {
  stop("Output data directory does not exist: ", out_dir)
}

## 1. Load consolidated phenotypic TSV ------------------------

# Try to locate the consolidated phenotypic file automatically
pheno_tsv <- list.files(
  out_dir,
  pattern = "adhd200.*phenotypics.*\\.tsv$",
  full.names = TRUE,
  ignore.case = TRUE
)

if (length(pheno_tsv) != 1L) {
  stop(
    "Could not uniquely locate the consolidated phenotypics TSV in ",
    out_dir, "\nFound: ", paste(basename(pheno_tsv), collapse = ", ")
  )
}

cat("Reading phenotypic TSV:\n  ", pheno_tsv, "\n", sep = "")
pheno_all <- read.delim(
  pheno_tsv,
  stringsAsFactors = FALSE,
  check.names = TRUE
)

## Normalise key column names: ScanDir.ID and Site -------------
id_col <- grep(
  "^ScanDir\\.ID$|^ScanDirID$|^ScanDir_Id$|^ScanDir\\.ID\\.$",
  names(pheno_all),
  ignore.case = TRUE,
  value = TRUE
)

if (length(id_col) != 1L) {
  stop("Could not identify ScanDir.ID column in phenotypic table. ",
       "Found candidates: ", paste(names(pheno_all), collapse = ", "))
}

if (id_col != "ScanDir.ID") {
  names(pheno_all)[names(pheno_all) == id_col] <- "ScanDir.ID"
}
pheno_all$ScanDir.ID <- as.character(pheno_all$ScanDir.ID)

site_col <- grep("^Site$", names(pheno_all), ignore.case = TRUE, value = TRUE)
if (length(site_col) == 1L && site_col != "Site") {
  names(pheno_all)[names(pheno_all) == site_col] <- "Site"
}

cat("Phenotypic table loaded: ",
    nrow(pheno_all), " subjects x ",
    ncol(pheno_all), " variables.\n\n", sep = "")

## 2. Index all AAL ROI time-series .1D files ------------------

aal_root <- file.path(out_dir, "AAL")
if (!dir.exists(aal_root)) {
  stop("AAL directory not found at ", aal_root)
}

# All AAL time-course files, e.g.
#   AAL/KKI/1018959/sfnwmrda1018959_session_1_rest_1_aal_TCs.1D
#   AAL/KKI/1018959/snwmrda1018959_session_1_rest_1_aal_TCs 2.1D
aal_files <- list.files(
  aal_root,
  pattern = "aal_TCs.*\\.1D$",
  full.names = TRUE,
  recursive = TRUE
)

if (length(aal_files) == 0L) {
  stop("No AAL ROI time-series .1D files found under ", aal_root)
}

# Helper: parse subject ID, site, and run from file path
parse_aal_path <- function(fp) {
  site_dir <- basename(dirname(dirname(fp)))  # e.g. "KKI"
  subj_dir <- basename(dirname(fp))          # e.g. "1018959"

  # Prefer folder name if it is purely numeric
  if (grepl("^[0-9]+$", subj_dir)) {
    sid <- subj_dir
  } else {
    # Fallback: extract longest run of digits from filename
    fn <- basename(fp)
    digits <- regmatches(fn, gregexpr("[0-9]{5,}", fn))[[1]]
    sid <- if (length(digits) >= 1L) digits[1] else NA_character_
  }

  # Extract run from "...rest_1..." etc., default to 1
  fn <- basename(fp)
  run_num <- suppressWarnings(as.integer(sub(".*rest_([0-9]+).*", "\\1", fn)))
  if (is.na(run_num)) run_num <- 1L

  data.frame(
    filepath   = fp,
    Site       = site_dir,
    ScanDir.ID = sid,
    run        = run_num,
    stringsAsFactors = FALSE
  )
}

aal_index <- bind_rows(lapply(aal_files, parse_aal_path))

# Keep only entries with a valid subject ID
aal_index <- aal_index %>%
  filter(!is.na(ScanDir.ID) & ScanDir.ID != "")

# Join phenotypics onto index
aal_index <- aal_index %>%
  mutate(ScanDir.ID = as.character(ScanDir.ID)) %>%
  left_join(pheno_all, by = "ScanDir.ID")

cat("Indexed ", nrow(aal_index), " AAL files.\n",
    "  Distinct subjects with AAL time series: ",
    length(unique(aal_index$ScanDir.ID)), "\n",
    "  With non-missing DX labels: ",
    sum(!is.na(aal_index$DX)), "\n\n", sep = "")

## 3. Reader for a single AAL .1D file ------------------------

# The .1D files are AFNI-style text matrices; we:
#   - drop blank lines and comment lines beginning with '#'
#   - drop a header row if it contains any alphabetic tokens
#   - read the remaining lines as a numeric matrix (timepoints x ROIs)
read_aal_1d <- function(path) {
  L <- readLines(path, warn = FALSE)

  # Remove empty and comment lines
  L <- L[!grepl("^\\s*$", L)]
  L <- L[!grepl("^\\s*#", L)]

  if (length(L) == 0L) {
    stop("No data lines in file: ", path)
  }

  # If first data line has alphabetic characters, treat it as header
  first_tokens <- strsplit(L[1], "[ \t]+")[[1]]
  if (any(grepl("[A-Za-z]", first_tokens))) {
    L <- L[-1]
    if (length(L) == 0L) {
      stop("No numeric data after header in file: ", path)
    }
  }

  DT <- data.table::fread(
    text = paste(L, collapse = "\n"),
    header = FALSE,
    data.table = FALSE,
    showProgress = FALSE
  )

  M <- as.matrix(DT)
  storage.mode(M) <- "numeric"
  M
}

## 4. Quick sanity check on one file --------------------------

example_fp <- aal_index$filepath[1]
ex_mat <- read_aal_1d(example_fp)

cat("Example AAL time-series file:\n  ", example_fp, "\n",
    "Matrix dimensions (timepoints x ROIs): ",
    paste(dim(ex_mat), collapse = " x "), "\n\n", sep = "")
```

```{r}
# Rebuild index starting from original aal_index
aal_index2 <- aal_index %>%
  mutate(
    basename = basename(filepath),
    prefix   = str_extract(basename, "^[a-z]+"),
    session  = as.integer(str_match(filepath, "session_([0-9]+)")[, 2]),
    rest     = as.integer(str_match(filepath, "rest_([0-9]+)")[, 2])
  ) %>%
  # keep only true time-series files
  filter(prefix %in% c("sfnwmrda", "snwmrda"))

# For each subject + site + session + rest, keep filtered if available, else unfiltered
aal_index_unique <- aal_index2 %>%
  group_by(ScanDir.ID, Site.x, session, rest) %>%  # use Site.x as you noted
  arrange(desc(prefix == "sfnwmrda"), .by_group = TRUE) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(
    session = ifelse(is.na(session), 1L, session),
    rest    = ifelse(is.na(rest),    1L, rest)
  )

cat("After filtering prefixes and collapsing runs:\n",
    "  Files:", nrow(aal_index_unique), "\n",
    "  Distinct subjects:", length(unique(aal_index_unique$ScanDir.ID)), "\n\n", sep = "")

# How many distinct (session, rest) runs per subject?
aal_index_unique %>%
  count(ScanDir.ID, name = "n_runs") %>%
  pull(n_runs) %>%
  table()

# By site: number of subjects and distribution of runs
aal_index_unique %>%
  count(Site.x, ScanDir.ID, name = "n_runs") %>%
  group_by(Site.x) %>%
  summarise(
    subjects      = n(),
    mean_runs     = mean(n_runs),
    median_runs   = median(n_runs),
    max_runs      = max(n_runs)
  )

aal_index_subj <- aal_index_unique %>%
  group_by(ScanDir.ID) %>%
  arrange(session, rest, .by_group = TRUE) %>%
  slice(1) %>%
  ungroup()

cat("Subject-level index:\n",
    "  Subjects:", nrow(aal_index_subj), "\n", sep = "")
```

```{r}
read_aal_1d <- function(path) {
  L <- readLines(path, warn = FALSE)

  # Drop empty & comment lines
  L <- L[!grepl("^\\s*$", L)]
  L <- L[!grepl("^\\s*#", L)]

  if (length(L) == 0L) {
    stop("No data lines in file: ", path)
  }

  # Drop first line if it contains any alphabetic characters (header)
  first_tokens <- strsplit(L[1], "[ \t]+")[[1]]
  if (any(grepl("[A-Za-z]", first_tokens))) {
    L <- L[-1]
    if (length(L) == 0L) {
      stop("No numeric data after header in file: ", path)
    }
  }

  # Read as numeric matrix
  DT <- data.table::fread(
    text = paste(L, collapse = "\n"),
    header = FALSE, data.table = FALSE, showProgress = FALSE
  )

  # Coerce columns to numeric; suppress warnings about e.g. empty strings
  DT[] <- lapply(DT, function(col) suppressWarnings(as.numeric(col)))

  M <- as.matrix(DT)

  # Drop first 2 metadata columns (File, Sub-brick), keep ROI columns
  if (ncol(M) >= 3L) {
    M <- M[, -(1:2), drop = FALSE]
  }

  M
}

compute_conn_matrix <- function(ts_mat, fisher = TRUE) {
  C <- suppressWarnings(cor(ts_mat, use = "pairwise.complete.obs"))
  if (fisher) atanh(C) else C
}

vec_upper <- function(C) {
  utri <- upper.tri(C)
  idx  <- which(utri, arr.ind = TRUE)   # <-- now 'idx' is definitely defined here
  vals <- C[utri]
  names(vals) <- paste0("edge_", idx[, 1], "_", idx[, 2])
  vals
}

```

```{r}
## --- Build a minimal subject index (ID + filepath only) -----------

aal_index_subj_small <- aal_index_subj %>%
  select(ScanDir.ID, filepath) %>%
  distinct()

cat("Subject-level index:\n",
    "  Subjects:", nrow(aal_index_subj_small), "\n\n", sep = "")

## --- Loop over subjects, compute connectivity features ------------

subject_conn_list <- pmap(
  list(
    id   = aal_index_subj_small$ScanDir.ID,
    path = aal_index_subj_small$filepath
  ),
  function(id, path) {
    ts_mat <- read_aal_1d(path)
    C      <- compute_conn_matrix(ts_mat, fisher = TRUE)
    v      <- vec_upper(C)
    c(ScanDir.ID = id, v)
  }
)

conn_df <- bind_rows(lapply(subject_conn_list, function(x) {
  as.data.frame(as.list(x), stringsAsFactors = FALSE)
}))

feature_cols <- setdiff(names(conn_df), "ScanDir.ID")

conn_df <- conn_df %>%
  mutate(
    ScanDir.ID = as.character(ScanDir.ID),
    ScanID_int = suppressWarnings(as.integer(ScanDir.ID))
  ) %>%
  mutate(across(all_of(feature_cols), as.numeric))

cat("Connectivity feature matrix:\n",
    "  Rows (subjects): ", nrow(conn_df), "\n",
    "  Columns (ID + edges): ", ncol(conn_df), "\n\n", sep = "")

## --- Prepare phenotypes with integer ID for matching --------------

pheno_all_int <- pheno_all %>%
  mutate(
    ScanDir.ID = as.character(ScanDir.ID),
    ScanID_int = suppressWarnings(as.integer(ScanDir.ID))
  )

## --- Merge on integer ID (fixes leading-zero mismatch) ------------

analysis_df <- conn_df %>%
  left_join(pheno_all_int, by = "ScanID_int", suffix = c(".imaging", ".pheno"))

# Keep imaging ID string as main ID, drop duplicate phenotypic ID column if present
if ("ScanDir.ID.pheno" %in% names(analysis_df)) {
  analysis_df <- analysis_df %>%
    select(-ScanDir.ID.pheno) %>%
    rename(ScanDir.ID = ScanDir.ID.imaging)
} else {
  # If only one ScanDir.ID is present, just rename to be safe
  analysis_df <- analysis_df %>%
    rename(ScanDir.ID = ScanDir.ID.imaging)
}

# If there are two Site columns, keep the string one (usually from imaging) and
# tuck the numeric one away as Site_num, then drop the duplicates.
if ("Site.x" %in% names(analysis_df) && "Site.y" %in% names(analysis_df)) {
  analysis_df <- analysis_df %>%
    rename(Site_name = Site.x,
           Site_num  = Site.y)
}

cat("Analysis dataset:\n",
    "  Rows (subjects): ", nrow(analysis_df), "\n",
    "  Columns: ", ncol(analysis_df), "\n\n", sep = "")

# Quick sanity checks
cat("DX distribution in analysis_df (non-missing):\n")
print(analysis_df %>% filter(!is.na(DX)) %>% count(DX))

cat("\nExample ScanDir.ID pairs (imaging vs phenotypic integer) for first 10 rows:\n")
print(analysis_df %>%
        select(ScanDir.ID, ScanID_int) %>%
        head(10))

analysis_qc <- analysis_df %>%
  mutate(
    QC_Athena = suppressWarnings(as.integer(QC_Athena)),
    QC_NIAK   = suppressWarnings(as.integer(QC_NIAK))
  ) %>%
  # keep Athena-passing subjects
  filter(QC_Athena == 1 | QC_NIAK == 1)

dim(analysis_qc)
```

\

## 1.2 Load Testing Data

```{r}
out_dir <- path.expand("~/Desktop/Projects/ADHD200/ADHD200_data")

if (!dir.exists(out_dir)) {
  stop("Output data directory does not exist: ", out_dir)
}

## 2. Index all AAL ROI time-series .1D files ------------------

aal_root <- file.path(out_dir, "Test_AAL")
if (!dir.exists(aal_root)) {
  stop("Test_AAL directory not found at ", aal_root)
}

aal_files <- list.files(
  aal_root,
  pattern = "aal_TCs.*\\.1D$",
  full.names = TRUE,
  recursive = TRUE
)

if (length(aal_files) == 0L) {
  stop("No Test AAL ROI time-series .1D files found under ", aal_root)
}

# Helper: parse subject ID, site, and run from file path
parse_aal_path <- function(fp) {
  site_dir <- basename(dirname(dirname(fp)))  # e.g. "KKI"
  subj_dir <- basename(dirname(fp))          # e.g. "1018959"

  # Prefer folder name if it is purely numeric
  if (grepl("^[0-9]+$", subj_dir)) {
    sid <- subj_dir
  } else {
    # Fallback: extract longest run of digits from filename
    fn <- basename(fp)
    digits <- regmatches(fn, gregexpr("[0-9]{5,}", fn))[[1]]
    sid <- if (length(digits) >= 1L) digits[1] else NA_character_
  }

  # Extract run from "...rest_1..." etc., default to 1
  fn <- basename(fp)
  run_num <- suppressWarnings(as.integer(sub(".*rest_([0-9]+).*", "\\1", fn)))
  if (is.na(run_num)) run_num <- 1L

  data.frame(
    filepath   = fp,
    Site       = site_dir,
    ScanDir.ID = sid,
    run        = run_num,
    stringsAsFactors = FALSE
  )
}

aal_index_test <- bind_rows(lapply(aal_files, parse_aal_path))

# Keep only entries with a valid subject ID
aal_index_test <- aal_index_test %>%
  filter(!is.na(ScanDir.ID) & ScanDir.ID != "")

# Join phenotypics onto index
aal_index_test <- aal_index_test %>%
  mutate(ScanDir.ID = as.character(ScanDir.ID)) %>%
  left_join(pheno_all, by = "ScanDir.ID")

cat("Indexed ", nrow(aal_index_test), " AAL files.\n",
    "  Distinct subjects with AAL time series: ",
    length(unique(aal_index_test$ScanDir.ID)), "\n",
    "  With non-missing DX labels: ",
    sum(!is.na(aal_index$DX)), "\n\n", sep = "")

## 3. Reader for a single AAL .1D file ------------------------

# The .1D files are AFNI-style text matrices; we:
#   - drop blank lines and comment lines beginning with '#'
#   - drop a header row if it contains any alphabetic tokens
#   - read the remaining lines as a numeric matrix (timepoints x ROIs)
read_aal_1d <- function(path) {
  L <- readLines(path, warn = FALSE)

  # Remove empty and comment lines
  L <- L[!grepl("^\\s*$", L)]
  L <- L[!grepl("^\\s*#", L)]

  if (length(L) == 0L) {
    stop("No data lines in file: ", path)
  }

  # If first data line has alphabetic characters, treat it as header
  first_tokens <- strsplit(L[1], "[ \t]+")[[1]]
  if (any(grepl("[A-Za-z]", first_tokens))) {
    L <- L[-1]
    if (length(L) == 0L) {
      stop("No numeric data after header in file: ", path)
    }
  }

  DT <- data.table::fread(
    text = paste(L, collapse = "\n"),
    header = FALSE,
    data.table = FALSE,
    showProgress = FALSE
  )

  M <- as.matrix(DT)
  storage.mode(M) <- "numeric"
  M
}

## 4. Quick sanity check on one file --------------------------

example_fp <- aal_index_test$filepath[1]
ex_mat <- read_aal_1d(example_fp)

cat("Example Test AAL time-series file:\n  ", example_fp, "\n",
    "Matrix dimensions (timepoints x ROIs): ",
    paste(dim(ex_mat), collapse = " x "), "\n\n", sep = "")
```

```{r}
# Rebuild index starting from original aal_index
aal_index2 <- aal_index_test %>%
  mutate(
    basename = basename(filepath),
    prefix   = str_extract(basename, "^[a-z]+"),
    session  = as.integer(str_match(filepath, "session_([0-9]+)")[, 2]),
    rest     = as.integer(str_match(filepath, "rest_([0-9]+)")[, 2])
  ) %>%
  # keep only true time-series files
  filter(prefix %in% c("sfnwmrda", "snwmrda"))

# For each subject + site + session + rest, keep filtered if available, else unfiltered
aal_index_unique <- aal_index2 %>%
  group_by(ScanDir.ID, Site.x, session, rest) %>%  # use Site.x as you noted
  arrange(desc(prefix == "sfnwmrda"), .by_group = TRUE) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(
    session = ifelse(is.na(session), 1L, session),
    rest    = ifelse(is.na(rest),    1L, rest)
  )

cat("After filtering prefixes and collapsing runs:\n",
    "  Files:", nrow(aal_index_unique), "\n",
    "  Distinct subjects:", length(unique(aal_index_unique$ScanDir.ID)), "\n\n", sep = "")

# How many distinct (session, rest) runs per subject?
aal_index_unique %>%
  count(ScanDir.ID, name = "n_runs") %>%
  pull(n_runs) %>%
  table()

# By site: number of subjects and distribution of runs
aal_index_unique %>%
  count(Site.x, ScanDir.ID, name = "n_runs") %>%
  group_by(Site.x) %>%
  summarise(
    subjects      = n(),
    mean_runs     = mean(n_runs),
    median_runs   = median(n_runs),
    max_runs      = max(n_runs)
  )

aal_index_subj <- aal_index_unique %>%
  group_by(ScanDir.ID) %>%
  arrange(session, rest, .by_group = TRUE) %>%
  slice(1) %>%
  ungroup()

cat("Subject-level index:\n",
    "  Subjects:", nrow(aal_index_subj), "\n", sep = "")
```

```{r}
read_aal_1d <- function(path) {
  L <- readLines(path, warn = FALSE)

  # Drop empty & comment lines
  L <- L[!grepl("^\\s*$", L)]
  L <- L[!grepl("^\\s*#", L)]

  if (length(L) == 0L) {
    stop("No data lines in file: ", path)
  }

  # Drop first line if it contains any alphabetic characters (header)
  first_tokens <- strsplit(L[1], "[ \t]+")[[1]]
  if (any(grepl("[A-Za-z]", first_tokens))) {
    L <- L[-1]
    if (length(L) == 0L) {
      stop("No numeric data after header in file: ", path)
    }
  }

  # Read as numeric matrix
  DT <- data.table::fread(
    text = paste(L, collapse = "\n"),
    header = FALSE, data.table = FALSE, showProgress = FALSE
  )

  # Coerce columns to numeric; suppress warnings about e.g. empty strings
  DT[] <- lapply(DT, function(col) suppressWarnings(as.numeric(col)))

  M <- as.matrix(DT)

  # Drop first 2 metadata columns (File, Sub-brick), keep ROI columns
  if (ncol(M) >= 3L) {
    M <- M[, -(1:2), drop = FALSE]
  }

  M
}

compute_conn_matrix <- function(ts_mat, fisher = TRUE) {
  C <- suppressWarnings(cor(ts_mat, use = "pairwise.complete.obs"))
  if (fisher) atanh(C) else C
}

vec_upper <- function(C) {
  utri <- upper.tri(C)
  idx  <- which(utri, arr.ind = TRUE)   # <-- now 'idx' is definitely defined here
  vals <- C[utri]
  names(vals) <- paste0("edge_", idx[, 1], "_", idx[, 2])
  vals
}

```

```{r}
## --- Build a minimal subject index (ID + filepath only) -----------

aal_index_subj_small <- aal_index_subj %>%
  select(ScanDir.ID, filepath) %>%
  distinct()

cat("Subject-level index:\n",
    "  Subjects:", nrow(aal_index_subj_small), "\n\n", sep = "")

## --- Loop over subjects, compute connectivity features ------------

subject_conn_list <- pmap(
  list(
    id   = aal_index_subj_small$ScanDir.ID,
    path = aal_index_subj_small$filepath
  ),
  function(id, path) {
    ts_mat <- read_aal_1d(path)
    C      <- compute_conn_matrix(ts_mat, fisher = TRUE)
    v      <- vec_upper(C)
    c(ScanDir.ID = id, v)
  }
)

conn_df <- bind_rows(lapply(subject_conn_list, function(x) {
  as.data.frame(as.list(x), stringsAsFactors = FALSE)
}))

feature_cols <- setdiff(names(conn_df), "ScanDir.ID")

conn_df <- conn_df %>%
  mutate(
    ScanDir.ID = as.character(ScanDir.ID),
    ScanID_int = suppressWarnings(as.integer(ScanDir.ID))
  ) %>%
  mutate(across(all_of(feature_cols), as.numeric))

cat("Connectivity feature matrix:\n",
    "  Rows (subjects): ", nrow(conn_df), "\n",
    "  Columns (ID + edges): ", ncol(conn_df), "\n\n", sep = "")

## --- Prepare phenotypes with integer ID for matching --------------

pheno_all_int <- pheno_all %>%
  mutate(
    ScanDir.ID = as.character(ScanDir.ID),
    ScanID_int = suppressWarnings(as.integer(ScanDir.ID))
  )

## --- Merge on integer ID (fixes leading-zero mismatch) ------------

analysis_df <- conn_df %>%
  left_join(pheno_all_int, by = "ScanID_int", suffix = c(".imaging", ".pheno"))

# Keep imaging ID string as main ID, drop duplicate phenotypic ID column if present
if ("ScanDir.ID.pheno" %in% names(analysis_df)) {
  analysis_df <- analysis_df %>%
    select(-ScanDir.ID.pheno) %>%
    rename(ScanDir.ID = ScanDir.ID.imaging)
} else {
  # If only one ScanDir.ID is present, just rename to be safe
  analysis_df <- analysis_df %>%
    rename(ScanDir.ID = ScanDir.ID.imaging)
}

# If there are two Site columns, keep the string one (usually from imaging) and
# tuck the numeric one away as Site_num, then drop the duplicates.
if ("Site.x" %in% names(analysis_df) && "Site.y" %in% names(analysis_df)) {
  analysis_df <- analysis_df %>%
    rename(Site_name = Site.x,
           Site_num  = Site.y)
}

cat("Analysis dataset:\n",
    "  Rows (subjects): ", nrow(analysis_df), "\n",
    "  Columns: ", ncol(analysis_df), "\n\n", sep = "")

# Quick sanity checks
cat("DX distribution in analysis_df (non-missing):\n")
print(analysis_df %>% filter(!is.na(DX)) %>% count(DX))

cat("\nExample ScanDir.ID pairs (imaging vs phenotypic integer) for first 10 rows:\n")
print(analysis_df %>%
        select(ScanDir.ID, ScanID_int) %>%
        head(10))

analysis_qc <- analysis_df %>%
  mutate(
    QC_Athena = suppressWarnings(as.integer(QC_Athena)),
    QC_NIAK   = suppressWarnings(as.integer(QC_NIAK))
  ) %>%
  # keep Athena-passing subjects
  filter(QC_Athena == 1 | QC_NIAK == 1)

dim(analysis_qc)
```

## 1.3 Pheno Data

```{r}
pheno_all %>%
  count(Site)

pheno_all %>%
  count(DX)

length(unique(pheno_all$ScanDir.ID))
summary(pheno_all$Age)
table(pheno_all$Gender, useNA = "ifany")

```

## EDA

```{r}
test_fp  <- aal_index_subj$filepath[1]
test_ts  <- read_aal_1d(test_fp)
dim(test_ts)    # expect T x 116
test_C   <- compute_conn_matrix(test_ts)
length(vec_upper(test_C))   # should be 6670 for AAL

```

```{r}
head(analysis_df[6670:6690])
```

```{r}
library(dplyr)
library(tidyr)

feature_cols <- grep("^edge_", names(analysis_df), value = TRUE)

# Dimensions
n_subj <- nrow(analysis_df)
p_feat <- length(feature_cols)
cat("Subjects:", n_subj, "  Connectivity features:", p_feat, "\n\n")

# Diagnosis distribution (DX is currently a character)
analysis_df %>%
  count(DX, sort = TRUE)

# Age, Gender, Handedness distributions
summary(analysis_df$Age)

table(analysis_df$Gender, useNA = "ifany")

summary(as.numeric(analysis_df$Handedness), useNA = "ifany")

# Site counts
analysis_df %>%
  count(Site, sort = TRUE)

```

```{r}
X <- as.matrix(analysis_df[, feature_cols])

# NA counts per feature (edge)
na_per_edge <- colSums(is.na(X))
summary(na_per_edge)
cat("Edges with any NA:", sum(na_per_edge > 0), "of", length(na_per_edge), "\n\n")

# NA counts per subject
na_per_subj <- rowSums(is.na(X))
summary(na_per_subj)
cat("Subjects with any NA:", sum(na_per_subj > 0), "of", length(na_per_subj), "\n\n")
```

```{r}
# Range of Fisher z values among all edges
summary(unlist(analysis_df[, feature_cols]))

# Range of corresponding correlations
summary(tanh(unlist(analysis_df[, feature_cols])))

# Variance of each edge
edge_sd <- apply(X, 2, sd, na.rm = TRUE)
summary(edge_sd)
cat("Edges with zero sd:", sum(edge_sd == 0, na.rm = TRUE), "\n")

```

```{r}
# Pick one subject
i <- 1
sub_ts <- read_aal_1d(aal_index_subj_small$filepath[i])

C_raw <- compute_conn_matrix(sub_ts, fisher = FALSE)
range(C_raw, na.rm = TRUE)    # should be in [-1, 1]

C_z <- compute_conn_matrix(sub_ts, fisher = TRUE)
range(C_z, na.rm = TRUE)      # Fisher z, unbounded

# Confirm link between z and r on this subject
all.equal(tanh(C_z), C_raw, tolerance = 1e-8)

```

```{r}
# identify connectivity columns
feature_cols <- grep("^edge_", names(analysis_df), value = TRUE)

# convert DX to numeric and factor labels
df_mod <- analysis_df %>%
  mutate(
    DX_num   = suppressWarnings(as.integer(DX)),
    DX_class = factor(
      DX_num,
      levels = c(0, 1, 2, 3),
      labels = c("TDC", "ADHD_C", "ADHD_I", "ADHD_HI")
    ),
    ADHD_bin = factor(ifelse(DX_num == 0, 0L, 1L), levels = c(0, 1))
  ) %>%
  filter(!is.na(DX_num) & DX_num %in% 0:3)

nrow(df_mod); length(feature_cols)
```

```{r}
# Class counts
df_mod %>% count(DX_class)

# Binary ADHD vs TDC
df_mod %>% count(ADHD_bin)

# Age / sex / site summaries
summary(df_mod$Age)
table(df_mod$Gender, useNA = "ifany")
df_mod %>% count(Site, sort = TRUE)
```

```{r}
set.seed(123)

# choose phenotypic predictors (tweak as you like)
pheno_predictors <- c("Age", "Gender", "Handedness", "Site")

df_pheno <- df_mod %>%
  select(DX_class, all_of(pheno_predictors)) %>%
  mutate(
    Gender     = factor(Gender),
    Site       = factor(Site),
    Handedness = as.numeric(Handedness)    # it's already numeric in your screenshot
  )

# K-fold CV for multinomial logistic
K <- 5
fold_id <- sample(rep(1:K, length.out = nrow(df_pheno)))

all_true <- df_pheno$DX_class
all_pred <- factor(NA, levels = levels(df_pheno$DX_class))

for (k in 1:K) {
  train_idx <- which(fold_id != k)
  test_idx  <- which(fold_id == k)
  
  fit_k <- multinom(DX_class ~ ., data = df_pheno[train_idx, ], trace = FALSE)
  pred_k <- predict(fit_k, newdata = df_pheno[test_idx, ])
  
  all_pred[test_idx] <- pred_k
}

cat("Phenotype-only multinomial logistic:\n")
cat("  Overall accuracy:", mean(all_pred == all_true), "\n")
print(table(True = all_true, Pred = all_pred))
mean(all_pred == all_true, na.rm = TRUE)
```
