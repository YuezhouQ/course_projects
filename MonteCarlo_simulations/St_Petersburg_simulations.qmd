---
title: "St. Petersburg Game Simulations"
format:
  html:
    embed-resources: true
    code-fold: false
editor: visual

references:
  - id: feller1971
    type: book
    author:
      - family: Feller
        given: William
    issued:
      - year: 1971
    title: "An Introduction to Probability Theory and Its Applications"
    volume: "2"
    publisher: Wiley
  - id: durrett2019
    type: book
    author:
      - family: Durrett
        given: Rick
    issued:
      - year: 2019
    title: "Probability: Theory and Examples"
    edition: "5"
    publisher: Cambridge University Press
  - id: csorgo1996
    type: article-journal
    author:
      - family: "Cs\u00f6rg\u0151"
        given: "S\u00e1ndor"
      - family: Simons
        given: Gordon D.
    issued:
      - year: 1996
    title: "A strong law of large numbers for trimmed sums, with applications to generalized St. Petersburg games"
    container-title: "Statistics & Probability Letters"
    volume: "26"
    issue: "1"
    page: "65-73"
    DOI: "10.1016/0167-7152(94)00253-3"
  - id: mori1976
    type: article-journal
    author:
      - family: Mori
        given: Toshio
    issued:
      - year: 1976
    title: "The strong law of large numbers when extreme terms are excluded from sums"
    container-title: "Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und Verwandte Gebiete"
    volume: "36"
    page: "189-194"
    DOI: "10.1007/BF00532544"
  - id: devroye2016
    type: article-journal
    author:
      - family: Devroye
        given: Luc
      - family: Lerasle
        given: Matthieu
      - family: Lugosi
        given: "G\u00e1bor"
      - family: Oliveira
        given: Roberto I.
    issued:
      - year: 2016
    title: "Sub-Gaussian mean estimators"
    container-title: "The Annals of Statistics"
    volume: "44"
    issue: "6"
    page: "2695-2725"
    DOI: "10.1214/16-AOS1440"
  - id: huber2009
    type: book
    author:
      - family: Huber
        given: Peter J.
      - family: Ronchetti
        given: Elvezio M.
    issued:
      - year: 2009
    title: "Robust Statistics"
    edition: "2"
    publisher: Wiley
  - id: resnick2007
    type: book
    author:
      - family: Resnick
        given: Sidney I.
    issued:
      - year: 2007
    title: "Heavy-Tail Phenomena: Probabilistic and Statistical Modeling"
    publisher: Springer
  - id: embrechts1997
    type: book
    author:
      - family: Embrechts
        given: Paul
      - family: "Kl\u00fcppelberg"
        given: Claudia
      - family: Mikosch
        given: Thomas
    issued:
      - year: 1997
    title: "Modelling Extremal Events: for Insurance and Finance"
    publisher: Springer
---

```{r}
library(stpetersburg) 
?simulate_stpetersburg_experiment
library(ggplot2) 
library(dplyr) 
library(tidyr)
library(scales)
library(knitr)
```

### 1. Introduction

The St. Petersburg game is a classical example where "infinite expectation" collides with our intuition about averages [@durrett2019]. In the usual formulation, we toss a fair coin until the first head appears. If the first head occurs on toss $k$, the payoff is $2^k$ dollars. The payoff $X$ therefore takes values in $\{2,4,8, \ldots\}$ with

$$
\mathbb{P}\left(X=2^k\right)=2^{-k}, \quad k \geq 1 .
$$

A quick computation shows

$$
\mathbb{E}[X]=\sum_{k=1}^{\infty} 2^k 2^{-k}=\sum_{k=1}^{\infty} 1=\infty,
$$

so the game has infinite expected value, even though no reasonable player would pay an arbitrarily large entry fee. This tension is the classical "St. Petersburg paradox." Indeed, the simulation below shows that the mean seems to be increasing as $n$ increases, though with a very slow rate.

```{r}
# Simulation parameters  
set.seed(123)   # set seed for reproducibility  
B <- 1000  
n_values <- c(100, 1000, 10000)  

# Run simulation experiment for each n and combine results  
sim_list <- lapply(n_values, function(n) {  
  simulate_stpetersburg_experiment(n = n, B = B)  
})  
sim_data <- do.call(rbind, sim_list)  

head(sim_list[[1]])
head(sim_list[[2]])
head(sim_list[[3]])
```

### 2. Simulation of the Scaled Average $A_n$

To study empirical behavior, denote $X_1, X_2, \ldots$ i.i.d. payoffs from repeated plays, with partial sums

$$
S_n=X_1+\cdots+X_n .
$$

Because $\mathbb{E}\left[X_1\right]=\infty$, the usual strong law of large numbers for $S_n / n$ fails. However, we can consider

$$
A_n=\frac{S_n}{n \log _2 n}, \text{ and } A_n \xrightarrow{\mathbb{P}} 1.
$$

Thus, a weak law with the $n \log _2 n$ scaling still holds, even though the mean is infinite. Here, we first examine the distribution of the scaled sample mean $A_n = \frac{S_n}{\,n \log_2 n\,}$ for different sample sizes. If the theory holds, $A_n$ should concentrate around 1 as $n$ increases, though very slowly. We simulate the empirical CDFs of $A_n$ below across 1000 simulations:

```{r}
summary(sim_data$A_n)

# Plot ECDFs of A_n for each sample size  
ggplot(sim_data, aes(x = A_n, color = factor(n))) +  
  stat_ecdf(linewidth = 1) +  
  labs(x = expression(A[n] == S[n] / (n * log[2]*n)),  
       color = "Sample size n",  
       title = "ECDF of Scaled Mean A_n",  
       subtitle = "St. Petersburg game simulations for n = 100, 1000, 10000") +  
  theme_minimal()  

# Zoom in
ggplot(sim_data, aes(x = A_n, color = factor(n))) +  
  stat_ecdf(linewidth = 1) +  
  xlim(0, 5) + 
  labs(x = expression(A[n] == S[n] / (n * log[2]*n)),  
       color = "Sample size n",  
       title = "ECDF of Scaled Mean A_n",  
       subtitle = "St. Petersburg game simulations for n = 100, 1000, 10000") +  
  theme_minimal()  
```

This confirms that $A_n$ does tend toward 1, but the convergence is extremely slow. Even at $n=10000$, there is substantial probability mass away from 1, reflecting the heavy-tail variability in sums. Next, we estimate $P(|A_n - 1| \le \varepsilon)$ for a few tolerance levels $\varepsilon$:

```{r}
# Define tolerance levels  
epsilons <- c(0.1, 0.25, 0.5)  

# Compute probability that A_n is within eps of 1 for each n  
prob_est <- expand.grid(n = n_values, eps = epsilons)  
prob_est$P <- NA  
for (i in seq_len(nrow(prob_est))) {  
  n_i   <- prob_est$n[i]  
  eps_i <- prob_est$eps[i]  
  # extract A_n values for this n  
  A_vals <- sim_data$A_n[ sim_data$n == n_i ]  
  # estimate probability  
  prob_est$P[i] <- mean( abs(A_vals - 1) <= eps_i )  
}  
print(prob_est)  
```

In theory, this probability should converge to 1 as $n$ increases. However, we see that even for $n=10000$, about 30% of 1000 runs had $A_n$ within $1\pm 0.1$, 57% were within $1\pm 0.25$, and 73% were within $1\pm 0.5$. Although the probability is increasing to 1 as $n$ increases, the rate of convergence is very slow.

### 3. Rare Extreme Payoffs and Outlier Counts

We investigate the occurrence of rare, extremely large payoffs that drive the variability. Using Borel-Cantelli, one can find constants $c>0$ such that the events

$$
\left\{X_k \geq c k \log _2 k\right\}
$$

occur infinitely often with probability one; each such "monster payoff" forces $S_k /\left(k \log _2 k\right)$ to make a large upward jump, preventing almost sure convergence. A convenient way to quantify these rare extremes is through the outlier count

$$
N_n(c)=\sum_{k \leq n} 1\left\{X_k \geq c k \log _2 k\right\} .
$$

This roughly counts how many data points are “unusually large” relative to the expected scale at that position. Since

$$
\mathbb{P}\left(X_k \geq c k \log _2 k\right) \approx \frac{\text { const }}{k \log k}
$$

one gets

$$
\mathbb{E}\left[N_n(c)\right] \asymp \frac{1}{c} \log \log n,
$$

so the expected number of extreme observations grows (very slowly) without bound. This logarithmic divergence underlies the failure of the strong law: with probability one, we see infinitely many huge payoffs that keep reshaping the average.

```{r}
# Compute average N_n(c) for each sample size  
avg_N <- aggregate(N_n ~ n, data = sim_data, FUN = mean)  
print(avg_N)  
```

We see average $N_n(10)$ grows slowly with $n$. Although in most runs for our $n$, only one extremely large outlier occurs, but the chance of seeing at least one such outlier keeps increasing with $n$. As we repeat our experiemnts, no matter how large $n$ gets, there will eventually be runs with new, huge payouts. This aligns with the idea that the strong law of large numbers fails here, that rare outliers continue to appear and prevent almost-sure convergence.

## 4. Contribution of the Maximum to the Sum

Let $$
M_n := \max_{1\le k\le n} X_k
\qquad\text{and}\qquad
\frac{M_n}{S_n}
$$ measure how dominant the largest observation is within the total.

A useful heuristic comes from the tail $\mathbb{P}(X\ge x)\asymp 1/x$ (regular variation with index $1$). The \`\`typical'' maximum scale satisfies $n\,\mathbb{P}(X\ge x)\approx 1$, which gives $M_n=O_{\mathbb{P}}(n)$. By contrast, the partial sums satisfy $S_n = \Theta_{\mathbb{P}}(n\log n)$ under the Feller normalization $A_n$ [@feller1971; @durrett2019]. Therefore, in typical runs $$
\frac{M_n}{S_n}
\approx \frac{n}{n\log n}
= \frac{1}{\log n},
$$ so the ratio tends to $0$ in probability (slowly). This predicts the empirical phenomenon that the right tail of $M_n/S_n$ becomes less prominent as $n$ grows.

### Histograms of $M_n/S_n$ across $n$

```{r}
# Derived quantities used throughout
sim_data$max_frac <- sim_data$M_n / sim_data$S_n
sim_data$R_n      <- (sim_data$S_n - sim_data$M_n) / (sim_data$n * log2(sim_data$n))

ggplot(sim_data, aes(x = max_frac)) +
  geom_histogram(bins = 25, color = "black", fill = "skyblue") +
  facet_wrap(~ n, nrow = 1) +
  scale_x_continuous(labels = percent_format(accuracy = 1)) +
  labs(x = expression(M[n]/S[n]),
       y = paste0("Frequency (out of ", B, " runs)"),
       title = "Maximum share of the sum: M_n / S_n",
       subtitle = "Tail mass shrinks with n because S_n grows like n log n while M_n grows like n") +
  theme_minimal()
```

```{r}
max_prob <- sim_data %>%
  group_by(n) %>%
  summarise(
    median = median(max_frac),
    p_gt_25 = mean(max_frac > 0.25),
    p_gt_50 = mean(max_frac > 0.50),
    p_gt_75 = mean(max_frac > 0.75),
    .groups = "drop"
  )

kable(max_prob, digits = 3,
      col.names = c("n", "Median(M_n/S_n)",
                    "P(M_n/S_n>0.25)",
                    "P(M_n/S_n>0.50)",
                    "P(M_n/S_n>0.75)"))
```

Even though $M_n/S_n$ typically decreases with $n$, the distribution retains a right tail: occasionally one very large payoff appears and contributes a sizable share of $S_n$. These occasional events are exactly what create the long right tail in the distribution of $A_n$ at finite $n$ and, at the path level, what drives the failure of almost-sure convergence.

------------------------------------------------------------------------

## 5. Removing the Maximum and the Meaning of $R_n$

Define the max-removed normalization

$$
R_n := \frac{S_n - M_n}{n\log_2 n}.
$$

Algebraically, $A_n - R_n = \frac{M_n}{n\log_2 n}$, so $R_n$ isolates the contribution from all terms except the single largest payoff. This provides a direct diagnostic for whether the instability of $A_n$ is coming from one dominating observation. A key theorem makes this interpretation precise: *light trimming* restores a strong law for the St.\~Petersburg game. In particular, removing the largest term yields $$
\frac{S_n - M_n}{n\log_2 n} \xrightarrow{\text{a.s.}} 1,
$$ as a special case of strong laws for trimmed sums [@csorgo1996; @mori1976]. Thus $R_n$ is designed to converge almost surely to the same constant that $A_n$ approaches only in probability.

### Monte Carlo: $A_n$ vs. $R_n$ across replications

```{r}
cmp_df <- sim_data %>%
  select(n, A_n, R_n) %>%
  pivot_longer(cols = c("A_n", "R_n"),
               names_to = "stat",
               values_to = "value") %>%
  mutate(stat = recode(stat,
                       A_n = "A_n = S_n/(n log2 n)",
                       R_n = "R_n = (S_n - M_n)/(n log2 n)"))

ggplot(cmp_df, aes(x = factor(n), y = value, fill = stat)) +
  geom_boxplot(outlier.alpha = 0.25, width = 0.70) +
  geom_hline(yintercept = 1, linetype = "dotted") +
  scale_y_continuous(trans = "log2") +
  labs(x = "Sample size n",
       y = "Value (log2 scale)",
       title = "Trimming the maximum stabilizes the Feller normalization",
       subtitle = "Log2 scale is used because A_n occasionally takes very large values") +
  theme_minimal() +
  theme(legend.position = "top")
```

### A pathwise decomposition plot (one long run)

```{r}
# One long trajectory to visualize spikes in A_n and stability of R_n
set.seed(123)

Nmax <- 50000
x <- r_stpetersburg(Nmax)

S <- cumsum(x)
M <- cummax(x)
n <- 2:Nmax

A_path <- S[n] / (n * log2(n))
R_path <- (S[n] - M[n]) / (n * log2(n))
C_path <- M[n] / (n * log2(n))  # the difference A_n - R_n

# Subsample for a cleaner plot
idx <- unique(round(exp(seq(log(2), log(Nmax), length.out = 2500))))
df_path <- data.frame(
  n = idx,
  A_n = A_path[idx - 1],
  R_n = R_path[idx - 1],
  max_term = C_path[idx - 1]
) %>%
  pivot_longer(cols = c("A_n", "R_n", "max_term"),
               names_to = "series",
               values_to = "value")

df_path$series <- factor(df_path$series,
                         levels = c("A_n", "R_n", "max_term"),
                         labels = c("A_n = S_n/(n log2 n)",
                                    "R_n = (S_n - M_n)/(n log2 n)",
                                    "A_n - R_n = M_n/(n log2 n)"))

ggplot(df_path, aes(x = n, y = value, color = series)) +
  geom_line(linewidth = 0.35, alpha = 0.9) +
  geom_hline(yintercept = 1, linetype = "dotted") +
  scale_x_log10() +
  coord_cartesian(ylim = c(0, 6)) +
  labs(x = "n (log scale)",
       y = NULL,
       title = "Pathwise meaning of R_n: remove the single largest payoff",
       subtitle = "Spikes in A_n align with spikes in M_n/(n log2 n); R_n stays near 1") +
  theme_minimal() +
  theme(legend.position = "top")
```

The pathwise plot makes the meaning of $R_n$ explicit: when $A_n$ jumps, the difference $A_n-R_n = M_n/(n\log_2 n)$ jumps with it. Removing the maximum removes these spikes. This matches the theoretical trimmed strong law: the \`\`bulk'' behaves law-of-large-numbers-like, while the occasional record payoff is responsible for almost-sure instability.

------------------------------------------------------------------------

## 6. Robust Estimators: Mean, Trimmed Mean, and Median-of-Means

Although $\mathbb{E}[X]=\infty$, one can still compare *finite-sample stability* of different location estimators:

-   **Sample mean** $$
    \bar X_n := \frac{1}{n}\sum_{i=1}^n X_i.
    $$

-   **10% trimmed mean (symmetric)**\
    Let $X_{(1)}\le \cdots \le X_{(n)}$ be the order statistics and let $k=\lfloor 0.1n\rfloor$. The symmetric trimmed mean is $$
    \bar X_n^{\mathrm{trim}} := \frac{1}{n-2k}\sum_{i=k+1}^{n-k} X_{(i)}.
    $$ (This matches `mean(x, trim = 0.1)` in base R.)

-   **Median-of-means (MoM)** with $K=10$ blocks\
    Partition $\{1,\dots,n\}$ into $K$ consecutive blocks of size $\approx n/K$, compute each block mean $\bar X^{(j)}$, and return $$
    \mathrm{MoM}_n := \mathrm{median}\{\bar X^{(1)},\dots,\bar X^{(K)}\}.
    $$ Median-of-means is a standard robust mean estimator with strong concentration guarantees under weak moment assumptions [@devroye2016].

### Boxplot comparison on a log scale

```{r}
#| label: robust-estimators-boxplot
est_data <- sim_data %>%
  select(n, mean, trimmed, median_of_means) %>%
  pivot_longer(cols = c("mean", "trimmed", "median_of_means"),
               names_to = "estimator",
               values_to = "value")

est_data$estimator <- factor(est_data$estimator,
                             levels = c("mean", "median_of_means", "trimmed"),
                             labels = c("Sample mean", "Median-of-means (10 blocks)", "10% trimmed mean"))

ggplot(est_data, aes(x = factor(n), y = value, fill = estimator)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.8)) +
  scale_y_continuous(trans = "log2") +
  labs(x = "Sample size n",
       y = "Estimator value (log2 scale)",
       title = "Robust estimators reduce variability under heavy tails") +
  theme_minimal() +
  theme(legend.position = "top")
```

```{r}
#| label: robust-estimators-summary
# Simple numeric summaries to accompany the boxplots
summ_est <- est_data %>%
  group_by(n, estimator) %>%
  summarise(
    median = median(value),
    iqr = IQR(value),
    q90 = quantile(value, 0.90),
    q99 = quantile(value, 0.99),
    .groups = "drop"
  )

kable(summ_est, digits = 3)
```

The ordinary mean is highly unstable: because a single extreme payoff can inflate $S_n$ dramatically, the distribution of $\bar X_n$ across runs has a heavy right tail. The two robust estimators stabilize by reducing sensitivity to the largest observations:

-   The **trimmed mean** is most stable here because the St.\~Petersburg distribution produces extremely large values that are rare but huge; discarding a fixed fraction of the largest values removes these rare extremes entirely. (Symmetric trimming also discards some of the smallest values, further reducing spread.) This comes with bias: it targets a \`\`typical'' scale rather than $\mathbb{E}[X]$.

-   The **median-of-means** remains stable because even if an extreme payoff contaminates one block mean, the median operator downweights that block as long as fewer than half of the blocks are contaminated. MoM is designed for heavy-tailed mean estimation and can achieve sub-Gaussian deviations under mild assumptions [@devroye2016], making it a principled alternative to trimming [@huber2009].

------------------------------------------------------------------------

## Appendix 

### A.1 Connection to Power Law Tails

Many of the phenomena above reflect general principles for heavy-tailed sums. Suppose $Y_1,Y_2,\dots$ are i.i.d. nonnegative with a Pareto-type tail $$
\mathbb{P}(Y>y) = y^{-\alpha}L(y),
$$ where $\alpha>0$ and $L$ is slowly varying (regular variation) [@resnick2007; @embrechts1997]. Let $S_n^Y=\sum_{i=1}^n Y_i$ and $M_n^Y=\max_{i\le n} Y_i$.

1.  **Finite-mean regime (**$\alpha>1$).\
    Then $\mathbb{E}[Y]<\infty$ and the classical strong law gives $S_n^Y/n\to \mathbb{E}[Y]$ almost surely. Also $M_n^Y=O_{\mathbb{P}}(n^{1/\alpha})$, so $$
    \frac{M_n^Y}{S_n^Y} \to 0 \quad\text{a.s.}
    $$ In this regime, the maximum is negligible relative to the sum.

2.  **Infinite-mean, \`\`one big jump'' regime (**$0<\alpha<1$).\
    Then $\mathbb{E}[Y]=\infty$ and both $M_n^Y$ and $S_n^Y$ are of order $n^{1/\alpha}$ (up to slowly varying factors). The sum is dominated by the largest few order statistics; consequently $M_n^Y/S_n^Y$ does **not** vanish and typically has a nondegenerate limiting distribution (related to Poisson--Dirichlet limits for normalized order statistics) [@resnick2007]. In this case, trimming only the single maximum may not stabilize the sum; stronger trimming is often needed.

3.  **Borderline case (**$\alpha=1$).\
    This is the analog of the St.\~Petersburg situation: the sum grows like $n\log n$ while the maximum is of order $n$, so $M_n^Y/S_n^Y\to 0$ in probability at a logarithmic rate. Nevertheless, rare record values still occur infinitely often and can prevent almost-sure convergence for certain normalizations. Trimmed-sum strong laws of the type used above describe precisely how much trimming is required to recover almost-sure limits [@mori1976; @csorgo1996].

### Acknowledgement: This project made use of ChatGPT-5 to assist with generating coding ideas, debugging, and refining the writing.
